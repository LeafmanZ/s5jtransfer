# set up source information for snowball source
source:
  bucket_name: "source-bucket-name"
  bucket_prefix: "" # ex1 "mysubdir/" ex2 "my_data/date_move/"
  region: "snow" # set to 'snow' if it is a snowball
  access_key: 'wjeiofjiewofjioewfq'
  secret_access_key: 'ewqfjoewiqjfklweqjfk'

# set up local information for src_sync_single.py
local:
  directory: "temp_files_for_transfer/" # this is depricated and no longer used

# set up destination information for cloud
destination:
  bucket_name: "destination-bucket-name"
  bucket_prefix: "" # "mysubdir/"
  region: "us-east-1"
  access_key: 'ewfewqfewqf'
  secret_access_key: 'ewfjkwenfwefwefp'
  delete_staging_data: True # set to False if you are moving from a performer to snowball
  staging_volumes: ['default'] # set to ['default'] to use prebuilt volume setup, otherwise use this example ['/home/MyDir1', '/home/ubuntu/MyOtherDir2']

# Configuration for managing data transfer from s3-edge to localstore based on size
transfer_settings:
  max_size_to_transfer_src2l: 52073738204 # Maximum total size (in bytes) of files to transfer at one time from source to localstore (or each individual volume)
  max_size_to_transfer_l2dest: 200 # NOT IMPLEMENTED: Maximum total size (in gigabytes) of files to transfer at one time from localstore to destination
  # dest_endpoint_url: 'desintation-bucket-name.s3-accelerate.amazonaws.com' # Endpoint URL for destination s3 bucket, if none set to 'no_endpoint', for legacy snowball use http + port #, otherwise https without port #
  dest_endpoint_url: ['no_endpoint']
  # dest_endpoint_url: ['https://999.999.999.12', 'https://999.999.999.13', 'https://999.999.999.14']
  src_endpoint_url: ['https://999.999.999.12'] # Endpoint URL for source s3 bucket, if none set to 'no_endpoint', for legacy snowball use http + port #, otherwise https without port #
  src_use_native_s3: True # Set to False to use s5cmd for src to local data transfers. True to use s3 api (safest).
